"""update ai_interviewee and past data

Revision ID: 48eca6561625
Revises: 51ed755c36cc
Create Date: 2026-01-13 15:12:39.091722

"""
from typing import Sequence, Union

from alembic import op
import sqlalchemy as sa
from sqlalchemy import inspect
from sqlalchemy.dialects import postgresql
import pgvector
from pgvector.sqlalchemy import Vector

# revision identifiers, used by Alembic.
revision: str = '48eca6561625'
down_revision: Union[str, Sequence[str], None] = '51ed755c36cc'
branch_labels: Union[str, Sequence[str], None] = None
depends_on: Union[str, Sequence[str], None] = None


def upgrade() -> None:
    """Upgrade schema."""
    # ### commands auto generated by Alembic - please adjust! ###
    op.create_table('ai_interviewees',
    sa.Column('id', sa.Integer(), nullable=False),
    sa.Column('hypothesis_id', sa.Integer(), nullable=True),
    sa.Column('company_type', sa.String(), nullable=True),
    sa.Column('market_segment', sa.String(), nullable=True),
    sa.Column('industry', sa.String(), nullable=True),
    sa.Column('poition', sa.String(), nullable=True),
    sa.Column('role', sa.String(), nullable=True),
    sa.Column('outreach_methods', sa.String(), nullable=True),
    sa.ForeignKeyConstraint(['hypothesis_id'], ['hypotheses.id'], ),
    sa.PrimaryKeyConstraint('id')
    )
    # Conditionally drop langchain tables if they exist
    connection = op.get_bind()
    inspector = inspect(connection)
    tables = inspector.get_table_names()
    
    if 'langchain_pg_embedding' in tables:
        # Drop index first if it exists
        try:
            op.drop_index('ix_cmetadata_gin', table_name='langchain_pg_embedding', postgresql_ops={'cmetadata': 'jsonb_path_ops'}, postgresql_using='gin')
        except Exception:
            pass  # Index might not exist
        op.drop_table('langchain_pg_embedding')
    
    if 'langchain_pg_collection' in tables:
        op.drop_table('langchain_pg_collection')
    op.add_column('past_data', sa.Column('embeddings', pgvector.sqlalchemy.vector.VECTOR(dim=1536), nullable=True))
    op.drop_constraint('past_data_team_id_fkey', 'past_data', type_='foreignkey')
    op.drop_column('past_data', 'user_personas')
    op.drop_column('past_data', 'hypothesis_embedding')
    op.drop_column('past_data', 'hypothesis')
    op.drop_column('past_data', 'hypothesis_score')
    op.drop_column('past_data', 'hypotheses_output_score')
    op.drop_column('past_data', 'hypothesis_evaluation')
    op.drop_column('past_data', 'team_id')
    op.drop_column('past_data', 'hypotheses_output')
    # ### end Alembic commands ###


def downgrade() -> None:
    """Downgrade schema."""
    # ### commands auto generated by Alembic - please adjust! ###
    op.add_column('past_data', sa.Column('hypotheses_output', sa.TEXT(), autoincrement=False, nullable=True))
    op.add_column('past_data', sa.Column('team_id', sa.VARCHAR(), autoincrement=False, nullable=True))
    op.add_column('past_data', sa.Column('hypothesis_evaluation', sa.TEXT(), autoincrement=False, nullable=True))
    op.add_column('past_data', sa.Column('hypotheses_output_score', sa.INTEGER(), autoincrement=False, nullable=True))
    op.add_column('past_data', sa.Column('hypothesis_score', sa.INTEGER(), autoincrement=False, nullable=True))
    op.add_column('past_data', sa.Column('hypothesis', sa.TEXT(), autoincrement=False, nullable=True))
    op.add_column('past_data', sa.Column('hypothesis_embedding', pgvector.sqlalchemy.vector.VECTOR(dim=1536), autoincrement=False, nullable=True))
    op.add_column('past_data', sa.Column('user_personas', postgresql.JSON(astext_type=sa.Text()), autoincrement=False, nullable=True))
    op.create_foreign_key('past_data_team_id_fkey', 'past_data', 'teams', ['team_id'], ['id'])
    op.drop_column('past_data', 'embeddings')
    op.create_table('langchain_pg_embedding',
    sa.Column('id', sa.VARCHAR(), autoincrement=False, nullable=False),
    sa.Column('collection_id', sa.UUID(), autoincrement=False, nullable=True),
    sa.Column('embedding', pgvector.sqlalchemy.vector.VECTOR(), autoincrement=False, nullable=True),
    sa.Column('document', sa.VARCHAR(), autoincrement=False, nullable=True),
    sa.Column('cmetadata', postgresql.JSONB(astext_type=sa.Text()), autoincrement=False, nullable=True),
    sa.ForeignKeyConstraint(['collection_id'], ['langchain_pg_collection.uuid'], name='langchain_pg_embedding_collection_id_fkey', ondelete='CASCADE'),
    sa.PrimaryKeyConstraint('id', name='langchain_pg_embedding_pkey')
    )
    op.create_index('ix_cmetadata_gin', 'langchain_pg_embedding', ['cmetadata'], unique=False, postgresql_ops={'cmetadata': 'jsonb_path_ops'}, postgresql_using='gin')
    op.create_table('langchain_pg_collection',
    sa.Column('uuid', sa.UUID(), autoincrement=False, nullable=False),
    sa.Column('name', sa.VARCHAR(), autoincrement=False, nullable=False),
    sa.Column('cmetadata', postgresql.JSON(astext_type=sa.Text()), autoincrement=False, nullable=True),
    sa.PrimaryKeyConstraint('uuid', name='langchain_pg_collection_pkey'),
    sa.UniqueConstraint('name', name='langchain_pg_collection_name_key', postgresql_include=[], postgresql_nulls_not_distinct=False)
    )
    op.drop_table('ai_interviewees')
    # ### end Alembic commands ###
